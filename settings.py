from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama

def initialize_settings():
    Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
    Settings.llm = Ollama(model="gemma:7b", request_timeout=120.0)
    Settings.chunk_size = 512
    Settings.chunk_overlap = 50